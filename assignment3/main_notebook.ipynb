{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b0739e0",
   "metadata": {},
   "source": [
    "## Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoawait True\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 120\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from preprocess.word_segmentor import segment_sentences_into_words\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7080b7e6",
   "metadata": {},
   "source": [
    "## Kiểm tra đường dẫn đến dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/viwik18/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13936514",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../data/raw/viwik18/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = os.listdir(data_path)\n",
    "filename_list.sort()\n",
    "filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfd410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the filename with the data file path to get the full path of each file\n",
    "filepath_list = [os.path.join(data_path, filename) for filename in filename_list]\n",
    "filepath_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first file with encoding UTF-8.\n",
    "# Replace error bytes with a placeholder character.\n",
    "with open(filepath_list[0], \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    text = f.read()\n",
    "text[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath_list[1], \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    text = f.read()\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40152990",
   "metadata": {},
   "source": [
    "### Vấn đề\n",
    "Khi tải dữ liệu Wikipedia dạng _shard_ (chia thành nhiều file nhỏ), một ký tự UTF-8 có thể bị “cắt ngang” ở cuối file đầu tiên và tiếp tục ở đầu file tiếp theo. UTF-8 dùng từ 1–4 byte cho mỗi ký tự, nên nếu chỉ đọc từng file riêng lẻ, Python sẽ gặp `UnicodeDecodeError` vì nó thấy một byte mở đầu mà thiếu byte tiếp theo.\n",
    "\n",
    "Cụ thể với `viwik18_aa`: byte cuối không đủ thông tin để tạo thành ký tự hợp lệ, dẫn đến lỗi decode.\n",
    "\n",
    "### Giải pháp\n",
    "Thay vì decode từng file một, ta nên nối tất cả các file ở mức byte (rb mode) rồi mới decode một lần. Như vậy các byte ký tự bị chia nhỏ ở ranh giới file sẽ được ghép lại đầy đủ, không còn gây lỗi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623302eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pattern to the data files\n",
    "file_pattern = os.path.join(data_path, \"viwik18_*\")\n",
    "\n",
    "# Search for all data files in the data directory\n",
    "files = sorted(glob.glob(file_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fecf1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bytes = b\"\".join(open(f, \"rb\").read() for f in files)\n",
    "text = all_bytes.decode(\"utf-8\")\n",
    "split_text = text.split(\"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc64da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14454af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each split, perform a .strip() on them\n",
    "cleaned_splits = [s.strip() for s in split_text if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Randomize 100 splits\n",
    "randomized_splits = random.sample(cleaned_splits, 100)\n",
    "randomized_splits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9968b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"NlpHUST/vi-word-segmentation\")\n",
    "\n",
    "nlp = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "sentences = segment_sentences_into_words(cleaned_splits, nlp)\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f23be",
   "metadata": {},
   "source": [
    "### Chuẩn bị các sentences để train model Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55086903",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sudo-code-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
